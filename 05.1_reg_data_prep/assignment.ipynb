{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e0c7b14a-e5aa-4abc-b48b-b8a9d20dacac",
      "metadata": {
        "id": "e0c7b14a-e5aa-4abc-b48b-b8a9d20dacac"
      },
      "source": [
        "### Lab IV: Linear Models\n",
        "### Answer all three sets of questions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf4826b0",
      "metadata": {
        "id": "bf4826b0"
      },
      "source": [
        "**Q0.** Please answer the following questions in your own words.\n",
        "\n",
        "1. What makes a model \"linear\"? \"Linear\" in what?\n",
        "2. How do you interpret the coefficient for a dummy/one-hot-encoded variable? (This is a trick question, and the trick involves how you handle the intercept of the model.) There's further explanation at the end of this document, if needed.\n",
        "3. Can linear regression be used for classification? Explain why, or why not.\n",
        "4. If you have a high accuracy on testing but low on testing, what might be the problem? It also might be you see a pattern in the residuals. \n",
        "5. Review this page: [Non-Linear with Linear](https://inria.github.io/scikit-learn-mooc/python_scripts/linear_regression_non_linear_link.html) What are two ways to incorporate nonlinear relationships between your target/response/dependent/outcome variable $y$ and your features/control/response/independent variables $x$?\n",
        "6. What is the interpretation of the intercept? A slope coefficient for a variable? The coefficient for a dummy/one-hot-encoded variable?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25bf83c6-ff44-42d6-9b33-8be1b945860d",
      "metadata": {
        "id": "25bf83c6-ff44-42d6-9b33-8be1b945860d"
      },
      "source": [
        "**Q1.** Load clean q1_clean: https://raw.githubusercontent.com/DS3001/linearRegression/refs/heads/main/data/Q1_clean.csv \n",
        "\n",
        "The data include\n",
        "\n",
        "- `Price` per night\n",
        "- `Review Scores Rating`: The average rating for the property\n",
        "- `Neighborhood `: The bourough of NYC. Note the space, or rename the variable.\n",
        "- `Property Type`: The kind of dwelling\n",
        "- `Room Type`: The kind of space being rented\n",
        "\n",
        "1. Compute the average prices and scores by `Neighborhood `; which borough is the most expensive on average? Create a kernel density plot of price and log price, grouping by `Neighborhood `.\n",
        "2. Regress price on `Neighborhood ` by creating the appropriate dummy/one-hot-encoded variables, without an intercept in the linear model. Compare the coefficients in the regression to the table from part 1. What pattern do you see? What are the coefficients in a regression of a continuous variable on one categorical variable?\n",
        "3. Repeat part 2, but leave an intercept in the linear model. How do you have to handle the creation of the dummies differently? What is the intercept? Interpret the coefficients. How can I get the coefficients in part 2 from these new coefficients?\n",
        "4. Split the sample 80/20 into a training and a test set. Run a regression of `Price` on `Review Scores Rating` and `Neighborhood `. What is the $R^2$ and RMSE on the test set? What is the coefficient on `Review Scores Rating`? What is the most expensive kind of property you can rent?\n",
        "5. Run a regression of `Price` on `Review Scores Rating` and `Neighborhood ` and `Property Type`. What is the $R^2$ and RMSE on the test set? What is the coefficient on `Review Scores Rating`? What is the most expensive kind of property you can rent?\n",
        "6. What does the coefficient on `Review Scores Rating` mean if it changes from part 4 to 5? Hint: Think about how multiple linear regression works.\n",
        "7. (Optional) We've included `Neighborhood ` and `Property Type` separately in the model. How do you interact them, so you can have \"A bedroom in Queens\" or \"A townhouse in Manhattan\". Split the sample 80/20 into a training and a test set and run a regression including that kind of \"property type X neighborhood\" dummy, plus `Review Scores Rating`. How does the slope coefficient for `Review Scores Rating`, the $R^2$, and the RMSE change? Do they increase significantly compares to part 5? Are the coefficients in this regression just the sum of the coefficients for `Neighbourhood ` and `Property Type` from 5? What is the most expensive kind of property you can rent?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95f22300-0180-4ed2-be8f-ed56cf4cd36b",
      "metadata": {
        "id": "95f22300-0180-4ed2-be8f-ed56cf4cd36b"
      },
      "source": [
        "**Q2.** This question is a case study for linear models. The data are about car prices. In particular, they include:\n",
        "\n",
        "  - `Price`, `Color`, `Seating_Capacity`\n",
        "  - `Body_Type`: crossover, hatchback, muv, sedan, suv\n",
        "  - `Make`, `Make_Year`: The brand of car and year produced\n",
        "  - `Mileage_Run`: The number of miles on the odometer\n",
        "  - `Fuel_Type`: Diesel or gasoline/petrol\n",
        "  - `Transmission`, `Transmission_Type`:  speeds and automatic/manual\n",
        "\n",
        "  1. Load `cars_hw.csv`. These data were really dirty, and I've already cleaned them a significant amount in terms of missing values and other issues, but some issues remain (e.g. outliers, badly skewed variables that require a log or arcsinh transformation) Note this is different than normalizing: there is a text below that explains further. Clean the data however you think is most appropriate.\n",
        "  2. Summarize the `Price` variable and create a kernel density plot. Use `.groupby()` and `.describe()` to summarize prices by brand (`Make`). Make a grouped kernel density plot by `Make`. Which car brands are the most expensive? What do prices look like in general?\n",
        "  3. Split the data into an 80% training set and a 20% testing set.\n",
        "  4. Make a model where you regress price on the numeric variables alone; what is the $R^2$ and `RMSE` on the training set and test set? Make a second model where, for the categorical variables, you regress price on a model comprised of one-hot encoded regressors/features alone (you can use `pd.get_dummies()`; be careful of the dummy variable trap); what is the $R^2$ and `RMSE` on the test set? Which model performs better on the test set? Make a third model that combines all the regressors from the previous two; what is the $R^2$ and `RMSE` on the test set? Does the joint model perform better or worse, and by home much?\n",
        "  5. Use the `PolynomialFeatures` function from `sklearn` to expand the set of numerical variables you're using in the regression. As you increase the degree of the expansion, how do the $R^2$ and `RMSE` change? At what point does $R^2$ go negative on the test set? For your best model with expanded features, what is the $R^2$ and `RMSE`? How does it compare to your best model from part 4?\n",
        "  6. For your best model so far, determine the predicted values for the test data and plot them against the true values. Do the predicted values and true values roughly line up along the diagonal, or not? Compute the residuals/errors for the test data and create a kernel density plot. Do the residuals look roughly bell-shaped around zero? Evaluate the strengths and weaknesses of your model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b67478ac-ad78-4a44-9720-583c71b8da14",
      "metadata": {
        "id": "b67478ac-ad78-4a44-9720-583c71b8da14"
      },
      "source": [
        "**Q3.**\n",
        "1. Find a dataset on a topic you're interested in. Some easy options are data.gov, kaggle.com, and data.world.\n",
        "2. Clean the data and do some exploratory data analysis on key variables that interest you. Pick a particular target/outcome variable and features/predictors.\n",
        "3. Split the sample into an ~80% training set and a ~20% test set.\n",
        "4. Run a few regressions of your target/outcome variable on a variety of features/predictors. Compute the RMSE on the test set.\n",
        "5. Which model performed the best, and why?\n",
        "6. What did you learn?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f6cf7bf",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "source": [
        "### Dummy Variable Trap \n",
        "\n",
        "\n",
        "In linear regression with categorical variables you should be careful of the Dummy Variable Trap. \n",
        "The Dummy Variable trap is a scenario in which the independent variables are multicollinear - a \n",
        "scenario in which two or more variables are highly correlated; in simple terms one variable can \n",
        "be predicted from the others. This can produce singularity of a model, meaning your model just won't \n",
        "work. Read about it here\n",
        "\n",
        "Idea is to use dummy variable encoding with drop_first=True, this will omit one column from each \n",
        "category after converting categorical variable into dummy/indicator variables. You WILL NOT lose \n",
        "and relevant information by doing that simply because your all point in dataset can fully be \n",
        "explained by rest of the features.\n",
        "\n",
        "Here is complete code on how you can do it for a \"jobs\" dataset\n",
        "\n",
        "So you have your X features:\n",
        "\n",
        "Age, Gender, Job, Classification \n",
        "\n",
        "And one numerical features that you are trying to predict:\n",
        "\n",
        "Wage\n",
        "\n",
        "First you need to split your initial dataset on input variables and prediction, \n",
        "assuming its pandas dataframe it would look like this:\n",
        "\n",
        "Input variables (your dataset is bit different but whole code remains the same, \n",
        "you will put every column from dataset in X, except one that will go to Y. pd.get_dummies \n",
        "works without problem like that - it will just convert categorical variables and it won't \n",
        "touch numerical):\n",
        "\n",
        "X = jobs[['Age','Gender','Job','Classification']]\n",
        "\n",
        "Prediction:\n",
        "\n",
        "Y = jobs['Wage']\n",
        "\n",
        "Convert categorical variable into dummy/indicator variables and drop one in each category:\n",
        "\n",
        "X = pd.get_dummies(data=X, drop_first=True)\n",
        "\n",
        "So now if you check shape of X (X.shape) with drop_first=True you will see that it has \n",
        "4 columns less - one for each of your categorical variables.\n",
        "\n",
        "You can now continue to use them in your linear model. For scikit-learn implementation it \n",
        "could look like this:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "505b7d47",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn import linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "    \n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = .20, random_state = 40)\n",
        "    \n",
        "regr = linear_model.LinearRegression() # Do not use fit_intercept = False if you have removed \n",
        "1 column after dummy encoding\n",
        "\n",
        "regr.fit(X_train, Y_train)\n",
        "predicted = regr.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c11ffcb",
      "metadata": {},
      "source": [
        "### Transformation versus Normalization\n",
        "\n",
        "Use a **log transformation** when your data is strictly positive and heavily skewed towards larger values (right-skewed), while an **arcsinh transformation** is better suited for data with both positive and negative values, or when you need to handle potential zero values; **normalize** your data when you need to scale all values to a similar range, typically between 0 and 1, and especially when using algorithms sensitive to feature scale, but not necessarily to achieve a **normal distribution**. \n",
        "Key points about each transformation: \n",
        "\n",
        "    Log transformation: \n",
        "\n",
        "    Best for positively skewed data with large variations in magnitude. \n",
        "\n",
        "Useful when analyzing relative changes rather than absolute differences. \n",
        "Cannot handle negative values. \n",
        "\n",
        "Arcsinh transformation:\n",
        "\n",
        "    Can handle both positive and negative data. \n",
        "\n",
        "Effective for data with extreme values on both ends of the spectrum. \n",
        "Often used when dealing with proportions or percentages near 0 or 1. \n",
        "\n",
        "Normalization: \n",
        "\n",
        "    Scales data to a common range, usually between 0 and 1. \n",
        "\n",
        "Useful when comparing features with different scales in machine learning algorithms. \n",
        "Does not necessarily transform the data distribution to be normal. \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".conda",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
